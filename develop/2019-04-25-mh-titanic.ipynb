{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intorduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is in this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the inputs which the model needs to run, please select one of the below for each input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magics & Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below table shows the version of libraries and packages used for running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Interactive matplotlib plot()\n",
    "#%matplotlib notebook\n",
    "\n",
    "# Autoreload packages before runs\n",
    "# https://ipython.org/ipython-doc/dev/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %install_ext http://raw.github.com/jrjohansson/version_information/master/version_information.py\n",
    "# ~/anaconda/bin/pip install version_information\n",
    "%load_ext version_information\n",
    "%version_information numpy, scipy, matplotlib, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Date and time\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Ipython imports\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports\n",
    "\n",
    "# Stats models\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from statsmodels.nonparametric import smoothers_lowess\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "# Sk-learn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizations\n",
    "sns.set() # matplotlib defaults\n",
    "\n",
    "# Any tweaks that normally go in .matplotlibrc, etc., should explicitly go here\n",
    "plt.rcParams['figure.figsize'] = (12, 12)\n",
    "\n",
    "# Silent mode\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the notebook the saved figures came from\n",
    "fig_prefix = \"../figures/2019-04-25-mh-titanic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/training/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:\n",
    "* 891 rows\n",
    "* 12 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Survived column is the target variable. If Suvival = 1 the passenger survived, otherwise he's dead. The is the variable we're going to predict.\n",
    "\n",
    "The other variables describe the passengers. They are the features.\n",
    "* PassengerId: and id given to each traveler on the boat\n",
    "* Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)\n",
    "* The Name of the passeger\n",
    "* The Sex\n",
    "* The Age\n",
    "* SibSp: number of siblings and spouses traveling with the passenger\n",
    "* Parch: number of parents and children traveling with the passenger\n",
    "* The ticket number\n",
    "* The ticket Fare\n",
    "* The cabin number\n",
    "* The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns.values:\n",
    "    print(col, ' :', data[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass, Sex, Embarked are categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age seems to have 177 missing values. let's impute this using the median age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = data['Age'].fillna(data['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Died'] = 1 - data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival count based on gender\n",
    "data.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='Bar', figsize=(12, 7),\n",
    "                                                          stacked=True, colors=['g', 'r'])\n",
    "plt.savefig(fig_prefix + '-Sex', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival ratio based on the gender\n",
    "data.groupby('Sex').agg('mean')[['Survived', 'Died']].plot(kind='Bar', figsize=(12, 7),\n",
    "                                                           stacked=True, colors=['g', 'r'])\n",
    "plt.savefig(fig_prefix + '-Sex-ratio', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots for correlating the survival with sex and age\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(x='Sex', y='Age', \n",
    "               hue='Survived', data=data,\n",
    "               split=True,\n",
    "               palette={0:'r', 1:'g'}\n",
    "              )\n",
    "plt.savefig(fig_prefix + '-age', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the chart above and validate by the following:\n",
    "* Women survive more than men, as depicted by the larger female green histogram\n",
    "\n",
    "Now, we see that:\n",
    "* The age conditions the survival for male passengers:\n",
    "    * Younger male tend to survive\n",
    "    * A large number of passengers between 20 and 40 succumb\n",
    "* The age doesn't seem to have a direct impact on the female survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(25, 12))\n",
    "plt.hist([data[data['Survived'] == 1]['Fare'], data[data['Survived'] == 0]['Fare']],\n",
    "         stacked=True, color=['g', 'r'],\n",
    "         bins=50, label = ['Survived', 'Dead'])\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Nubmer of passerngers')\n",
    "plt.legend()\n",
    "plt.savefig(fig_prefix + '-fare-dist', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 7))\n",
    "ax = plt.subplot()\n",
    "\n",
    "ax.scatter(data[data['Survived'] == 1]['Age'], data[data['Survived'] == 1]['Fare'],\n",
    "           c='g', s=data[data['Survived'] == 1]['Fare'])\n",
    "\n",
    "ax.scatter(data[data['Survived'] == 0]['Age'], data[data['Survived'] == 0]['Fare'],\n",
    "           c='r', s=data[data['Survived'] == 0]['Fare'])\n",
    "plt.savefig(fig_prefix + '-fare-scatter', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the circles is proportional to the ticket fare.\n",
    "\n",
    "On the x-axis, we have the ages and the y-axis, we consider the ticket fare.\n",
    "\n",
    "We can observe different clusters:\n",
    "1. Large green dots between x=20 and x=45: adults with the largest ticket fares\n",
    "2. Small red dots between x=10 and x=45, adults from lower classes on the boat\n",
    "3. Small greed dots between x=0 and x=7: these are the children that were saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.set_ylabel('Average fare')\n",
    "(data.groupby('Pclass')['Fare'].mean()).plot(kind='bar', figsize=(25, 7), ax=ax)\n",
    "plt.savefig(fig_prefix + '-fare-class', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 7))\n",
    "sns.violinplot(x='Embarked', y='Fare', hue='Survived', data=data,\n",
    "               split=True, palette={0: 'r', 1: 'g'})\n",
    "plt.savefig(fig_prefix + '-embared-fare', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the embarkation C have a wider range of fare tickets and therefore the passengers who pay the highest prices are those who survive.\n",
    "\n",
    "We also see this happening in embarkation S and less in embarkation Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that asserts whether or not a feature has been processed.\n",
    "def status(feature):\n",
    "    print('Processing', feature, ': ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for combining the train and the test data\n",
    "def get_combined_data():\n",
    "    # reading the train data\n",
    "    train = pd.read_csv('../data/training/train.csv')\n",
    "    \n",
    "    # reading the test data\n",
    "    test = pd.read_csv('../data/training/test.csv')\n",
    "    \n",
    "    # extracting and removing the target from the training data\n",
    "    targets = train.Survived\n",
    "    train.drop(['Survived'], 1, inplace=True)\n",
    "    \n",
    "    # merging train data and test data for future feature engineering \n",
    "    # we'll also remove the PassengerID since this is not an informative feature\n",
    "    combined = train.append(test)\n",
    "    combined.reset_index(inplace=True)\n",
    "    combined.drop(['index', 'PassengerId'], inplace=True, axis=1)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = get_combined_data()\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = set()\n",
    "for name in data['Name']:\n",
    "    titles.add(name.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dic ={\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Royalty\",\n",
    "    \"Don\": \"Royalty\",\n",
    "    \"Sir\" : \"Royalty\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Master\",\n",
    "    \"Lady\" : \"Royalty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles():\n",
    "    # We extract title from each name\n",
    "    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated title\n",
    "    # we map each title\n",
    "    combined['Title'] = combined.Title.map(title_dic)\n",
    "    status('Title')\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = get_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "combined[combined.Title.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is indeed a NaN value in the line 1305. In fact the corresponding name is Oliva y Ocana, Dona. Fermina.\n",
    "\n",
    "This title was not encoutered in the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing ages in train set\n",
    "print(combined.iloc[:891].Age.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing ages in test set\n",
    "print(combined.iloc[891:].Age.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping\n",
    "grouped_train = combined.iloc[:891].groupby(['Sex', 'Pclass', 'Title'])\n",
    "grouped_median_train = grouped_train.median()\n",
    "grouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n",
    "grouped_median_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_age(row):\n",
    "    condition = (\n",
    "        (grouped_median_train['Sex'] == row['Sex']) & \n",
    "        (grouped_median_train['Title'] == row['Title']) & \n",
    "        (grouped_median_train['Pclass'] == row['Pclass'])\n",
    "    ) \n",
    "    return grouped_median_train[condition]['Age'].values[0]\n",
    "\n",
    "\n",
    "def process_age():\n",
    "    global combined\n",
    "    # a function that fills the missing values of the Age variable\n",
    "    combined['Age'] = combined.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n",
    "    status('age')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_age()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_names():\n",
    "    global combined\n",
    "    # we clean the Name variable\n",
    "    combined.drop('Name', axis=1, inplace=True)\n",
    "    \n",
    "    # encoding in dummy variable\n",
    "    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n",
    "    combined = pd.concat([combined, titles_dummies], axis=1)\n",
    "    \n",
    "    # removing the title variable\n",
    "    combined.drop('Title', axis=1, inplace=True)\n",
    "    \n",
    "    status('names')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fares():\n",
    "    global combined\n",
    "    # there's one missing fare value - replacing it with the mean.\n",
    "    combined.Fare.fillna(combined.iloc[:891].Fare.mean(), inplace=True)\n",
    "    status('fare')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_fares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_embarked():\n",
    "    global combined\n",
    "    # two missing embarked values - filling them with the most frequent one in the train  set(S)\n",
    "    combined.Embarked.fillna('S', inplace=True)\n",
    "    # dummy encoding \n",
    "    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')\n",
    "    combined = pd.concat([combined, embarked_dummies], axis=1)\n",
    "    combined.drop('Embarked', axis=1, inplace=True)\n",
    "    status('embarked')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_embarked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cabin, test_cabin = set(), set()\n",
    "\n",
    "for c in combined.iloc[:891]['Cabin']:\n",
    "    try:\n",
    "        train_cabin.add(c[0])\n",
    "    except:\n",
    "        train_cabin.add('U')\n",
    "        \n",
    "for c in combined.iloc[891:]['Cabin']:\n",
    "    try:\n",
    "        test_cabin.add(c[0])\n",
    "    except:\n",
    "        test_cabin.add('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_cabin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any cabin letter in the test set that is not present in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cabin():\n",
    "    global combined    \n",
    "    # replacing missing cabins with U (for Uknown)\n",
    "    combined.Cabin.fillna('U', inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    combined['Cabin'] = combined['Cabin'].map(lambda c: c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(combined['Cabin'], prefix='Cabin')    \n",
    "    combined = pd.concat([combined, cabin_dummies], axis=1)\n",
    "\n",
    "    combined.drop('Cabin', axis=1, inplace=True)\n",
    "    status('cabin')\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function replaces NaN values with U (for Unknow). It then maps each Cabin value to the first letter. Then it encodes the cabin values using dummy encoding again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_cabin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sex():\n",
    "    global combined\n",
    "    # mapping string values to numerical one \n",
    "    combined['Sex'] = combined['Sex'].map({'male':1, 'female':0})\n",
    "    status('Sex')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_sex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pclass():\n",
    "    \n",
    "    global combined\n",
    "    # encoding into 3 categories:\n",
    "    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix=\"Pclass\")\n",
    "    \n",
    "    # adding dummy variable\n",
    "    combined = pd.concat([combined, pclass_dummies],axis=1)\n",
    "    \n",
    "    # removing \"Pclass\"\n",
    "    combined.drop('Pclass',axis=1,inplace=True)\n",
    "    \n",
    "    status('Pclass')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_pclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTicket(ticket):\n",
    "    ticket = ticket.replace('.', '')\n",
    "    ticket = ticket.replace('/', '')\n",
    "    ticket = ticket.split()\n",
    "    ticket = map(lambda t : t.strip(), ticket)\n",
    "    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "    if len(ticket) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = set()\n",
    "for t in combined['Ticket']:\n",
    "    tickets.add(cleanTicket(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tickets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticket():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip(), ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "    \n",
    "\n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')\n",
    "    combined = pd.concat([combined, tickets_dummies], axis=1)\n",
    "    combined.drop('Ticket', inplace=True, axis=1)\n",
    "\n",
    "    status('Ticket')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_ticket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family\n",
    "This part includes creating new variables based on the size of the family (the size is by the way, another variable we create).\n",
    "\n",
    "This creation of new variables is done under a realistic assumption: Large families are grouped together, hence they are more likely to get rescued than people traveling alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_family():\n",
    "    \n",
    "    global combined\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n",
    "    combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n",
    "    \n",
    "    status('family')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = process_family()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
